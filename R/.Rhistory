constrOptim
optim
?optim
optim
?constrOptim
class(mean())
class(mean)
as.function("mean")(1:3)
as.function("mean")
?as.function
optim
?match.arg
try = match.arg("mean")
try = match.arg(c("mean"))
try = match.arg(c("mean", "median"))
method = c("mean", "median")
method <- arg.match(method)
method <- match.arg(method)
?km
library(DiceKriging)
?km
km
?optim
###########################################################################
#' Subgradient Optimisation
#'
#' Function for minimising piecewise differentiable objective functions.
#'
#' @param theta Initial value.
#' @param fn Function to optimise.
#' @param df Subgradient of the objective function.
#' @param method Method to be used for optimisation. Default is subgradient
#' method.
#' @param tol Relative convergence tolerance
#' @param maxit Maximum number of iterations if relative convergence is not
#' reached.
#' @param acl Flag for accelerated methods. Default is false.
#' @param prox Proximal operator for proximal gradient method. Default is
#' NULL.
#' @param theta_it Iterative theta for coordinate descent algorithm. Default
#' is NULL.
#' @return The function returns a list with following components
#' \item{par}{The best set of parameters}
#' \item{value}{The value of the function corresponding to best set of parameters.}
#' \item{convergence}{Convergence message.}
#' @return
SubgradOptim = function(theta, fn, df, method, tol = 1e-5, maxit = 100, acl = NULL,
prox = NULL, theta_it = NULL){
# Sub-gradient optimisation.
sg = function(theta, fn, df, ts) {
fx = fn(theta)
par = theta
value = fx
for (t in ts) {
theta = theta - t * df(x)
fx = f(theta)
if(fx < value) {
par = theta
value = fx
}
}
par
}
# Proximal gradient optimisation.
pg = function(theta, fn, df, prox, ts) {
for (t in ts) par = prox(t, theta - t * df(x))
par
}
# Co-ordinate descent optimisation.
cd = function(theta, fn, theta_it, maxit = 100) {
for (j in 1:maxit) {
theta.last = theta
for(i in 1:length(theta)) {
theta[i] = theta_it(theta, i)
}
if(sum(abs(theta.last - theta)) < 0.00001)
break
}
theta[abs(theta) < 0.00001] = 0
theta
}
}
#' Sequence of step sizes for optimization.
#'
#' Function to generate stepsize for sub-gradient optimization and proximal-gradient optimization
#' @param t Starting value.
#' @param m Number of constant steps.
#' @param n Number of diminishing steps.
#' @return The sequence of stepsize
#' @export
opt_ts = function(t, m, n)
if (n != 0) {
c(rep(t, m), t / (1:n))
} else {
rep(t, m)
}
library(devtools)
install_github("tathagatabasu/SubgradOptim")
library(SubgradOptim)
x = matrix(data = rnorm(1200), nrow = 50, ncol = 6)
b = as.matrix(rep(c(-3,-2,-1,1,2,3), 1))
er = as.matrix(rnorm(50))
y = x %*% b + er
lambda = 2
###########################################################################
square_f = function(x, y, beta)
sum((y - x %*% beta)^2) / (2 * nrow(x))
square_df = function(x, y, beta)
-t(x) %*% (y - x %*% beta) / nrow(x)
lasso_f = function(lambda, beta) lambda * sum(abs(beta))
lasso_df = function(lambda, beta) lambda * sign(beta)
square_lasso_f = function(lambda, x, y, beta)
square_f(x, y, beta) + lasso_f(lambda, beta)
square_lasso_df = function(lambda, x, y, beta)
square_df(x, y, beta) + lasso_df(lambda, beta)
prox = function(lambda) function(t, x) sign(x) * pmax(0, abs(x) - lambda * t)
soft  = function(lambda, wt)function(x, i) sign(x) * max(0, abs(x) - lambda * wt[i])
s = soft(lambda, rep(1,6))
st_f = function(i, x, y, beta)
t(x[,i]) %*% (y - x[,-i] %*% beta[-i]) / (t(x[,i]) %*% x[,i])
v = function(i, beta) st_f(i, x, y, beta)
theta_it = function(beta, i)s(v(i, beta), i)
fn = function(beta) square_lasso_f(lambda, x, y, beta)
df = function(beta) square_lasso_df(lambda, x, y, beta)
#################################################################################
SubgradOptim(rep(0,6), fn, df, "cd", tol = 1e-5, maxit = 10000, h = 0.001,
NULL, acl = FALSE, theta_it = theta_it)
#################################################################################
SubgradOptim(rep(0,6), fn, df, "sg", tol = 1e-5, maxit = 10000, h = 0.001,
NULL, acl = FALSE, theta_it = theta_it)
#################################################################################
SubgradOptim(rep(0,6), fn, df, "pg", tol = 1e-5, maxit = 10000, h = 0.001,
NULL, acl = FALSE, theta_it = theta_it)
#################################################################################
SubgradOptim(rep(0,6), fn, df, "pg", tol = 1e-5, maxit = 10000, h = 0.001,
prox(lambda), acl = FALSE, theta_it = theta_it)
