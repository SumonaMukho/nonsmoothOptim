constrOptim
optim
?optim
optim
?constrOptim
class(mean())
class(mean)
as.function("mean")(1:3)
as.function("mean")
?as.function
optim
?match.arg
try = match.arg("mean")
try = match.arg(c("mean"))
try = match.arg(c("mean", "median"))
method = c("mean", "median")
method <- arg.match(method)
method <- match.arg(method)
?km
library(DiceKriging)
?km
km
?optim
###########################################################################
#' Subgradient Optimisation
#'
#' Function for minimising piecewise differentiable objective functions.
#'
#' @param theta Initial value.
#' @param fn Function to optimise.
#' @param df Subgradient of the objective function.
#' @param method Method to be used for optimisation. Default is subgradient
#' method.
#' @param tol Relative convergence tolerance
#' @param maxit Maximum number of iterations if relative convergence is not
#' reached.
#' @param acl Flag for accelerated methods. Default is false.
#' @param prox Proximal operator for proximal gradient method. Default is
#' NULL.
#' @param theta_it Iterative theta for coordinate descent algorithm. Default
#' is NULL.
#' @return The function returns a list with following components
#' \item{par}{The best set of parameters}
#' \item{value}{The value of the function corresponding to best set of parameters.}
#' \item{convergence}{Convergence message.}
#' @return
SubgradOptim = function(theta, fn, df, method, tol = 1e-5, maxit = 100, acl = NULL,
prox = NULL, theta_it = NULL){
# Sub-gradient optimisation.
sg = function(theta, fn, df, ts) {
fx = fn(theta)
par = theta
value = fx
for (t in ts) {
theta = theta - t * df(x)
fx = f(theta)
if(fx < value) {
par = theta
value = fx
}
}
par
}
# Proximal gradient optimisation.
pg = function(theta, fn, df, prox, ts) {
for (t in ts) par = prox(t, theta - t * df(x))
par
}
# Co-ordinate descent optimisation.
cd = function(theta, fn, theta_it, maxit = 100) {
for (j in 1:maxit) {
theta.last = theta
for(i in 1:length(theta)) {
theta[i] = theta_it(theta, i)
}
if(sum(abs(theta.last - theta)) < 0.00001)
break
}
theta[abs(theta) < 0.00001] = 0
theta
}
}
#' Sequence of step sizes for optimization.
#'
#' Function to generate stepsize for sub-gradient optimization and proximal-gradient optimization
#' @param t Starting value.
#' @param m Number of constant steps.
#' @param n Number of diminishing steps.
#' @return The sequence of stepsize
#' @export
opt_ts = function(t, m, n)
if (n != 0) {
c(rep(t, m), t / (1:n))
} else {
rep(t, m)
}
